= Configure AI toosl in Red Hat OpenShift Dev Spaces (Bonus Lab)

== Introduction

This Lab provides the opportunity to explore how Red Hat Advanced Developer Suite integrates with other products and third-party tools to bring AI into the developers' daily activities, but also how other developer tools, such as RH OpenShift Dev Spaces, provide an opportunity to adopt AI practices. 

== Objectives
This Lab will showcase how to configure an AI assistant and expand the AI assistant capabilities by configuring a Tavily MCP server.

Let's get started.

== Scenario
Now, imagine for a moment that you are part of the Python AI Development team. Your team has already built the AI agent built but now is your time to learn about this implementation.


== Getting started

* Access the Red Hat OpenShift Dev Spaces from the component already created in Red Hat Developer Hub:

** Go back to {rhdh_url/catalog/default/component/ai-agent}[Red Hat Developer Hub UI^]

** Login with your user credentials:

 *** *Username*: {rhdh_user}
 *** *Password*: {rhdh_user_password}

** Next, click on the component's *ai-agent*
** From the component's overview, *click on* the *Red Hat OpenShift Dev Spaces*
** Click on the *OpenShift Dev spaces* link from the *component overview* screen 

image:rhads-ai/rhads/rhdh-devspaces-click.png[width=100%]


** Login with your user credentials:

 *** *Username*: {devspaces_user}
 *** *Password*: {devspaces_user_password}


== Configure the *AI Assistant in Red Hat OpenShift Dev Spaces*
We want to take advantage of an AI assistant using OpenShift Dev Spaces. For this Lab, we'll use *https://www.continue.dev[Continue^]*. However, many organizations might use others, such as *Claude from Anthropic *, *Microsoft Copilot*.
In many organizations, developers must use an AI Assistant to increase productivity. Learning about AI assistants is a key part of AI and how this tool can be leverage within Red Hat products.

* In *Red Hat OpenShift Dev Spaces*, click on *Extensions*.

image:rhads-ai/rhdevspaces/devspaces-extension.png[width=15%]

* Search for the *continue* extension and select the option: *Install Release version*

image:rhads-ai/rhdevspaces/devspaces-continue-install-release.png[width=60%]

* Confirm on the *Trust the Publisher and Install* button.
image:rhads-ai/rhdevspaces/devspaces-trust.png[width=60%]

* After a successful installation, you will see a screen similar to this:

image:rhads-ai/rhdevspaces/devspaces-continue-page.png[width=100%]

== Configure the *LLM in the AI Assistant*

* Next, click on the *continue* icon.

image:rhads-ai/rhdevspaces/devspaces-continue-icon.png[width=40%]


* Next, click on the *Add Chat model* option, and click on *config file*.

image:rhads-ai/rhdevspaces/devspaces-continue-config-assistant.png[width=80%]

* You will see a new file created called: *config.yaml*

image:rhads-ai/rhdevspaces/devspaces-continue-config-yaml.png[width=100%]

* Copy the following content and past it to replace the *whole current content*:

** Copy the content provided, by clicking on the icon:

+
image:rhads-ai/rhads/rhdh-copy-icon.png[width=20%]


[source,bash,role=execute,subs=attributes+]
----
name: Local Assistant
version: 1.0.0
schema: v1
models:
  - name: llama-3-2-3b
    provider: openai
    model: llama-3-2-3b
    apiBase: 
    apiKey: ""
    roles:
      - chat
      - edit
      - apply
tabAutocompleteModel:
  title: RamaLama (AutoComplete)
  provider: custom
  model: default
allowAnonymousTelemetry: false
context:
  - provider: code
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase
----

**  If needed, verify the solution: https://github.com/redhat-ads-tech/rhads-enablement-l3/blob/main/content/modules/ROOT/solutions/rhads-ai/rhdevspaces/config.yaml[config.yaml AI assistant config file^]

* *Paste* it into the config.yaml file created by continue.

The next step will add the Model's *apiBase* and *apiKey* from the *Internal MaaS Demo from the AI BU*.


=== Get your API key to access the self-hosted Model
When configuring an AI assistant, it needs an LLM to work. Organizations might have their own self-hosted models or remote models. We use the Model as a Service portal for this Lab to access an LLM.

* Access the following URL and follow the steps to sign in with your *Red Hat account*.
link:https://maas.apps.prod.rhoai.rh-aiservices-bu.com/[Maas website,window='_blank']

*Note:* The link:https://maas.apps.prod.rhoai.rh-aiservices-bu.com/[Maas website,window='_blank'] is not an official Red Hat service. For Red Hat associate internal demo purposes only, provided 'as-is' without support or SLA. The intended purpose is to test the connectivity of Red Hat products to models that customers may use. The models are provided for this limited purpose.


** Click on *Sign in*

image:rhads-ai/rhdevspaces/maas-sign-in.png[width=60%]

** Click on *Authenticate with RH SSO*

image:rhads-ai/rhdevspaces/maas-authrh.png[width=80%]

** Click on *Google* to sign in to *RHOAI*, where the model is deployed with your account.

image:rhads-ai/rhdevspaces/maas-rhoai.png[width=60%]

** Click on applications

** Click on the model *llama-3-2-3b*

image:rhads-ai/rhdevspaces/maas-model-select.png[width=100%]


** Copy the *API Key* and paste it into the *config.yaml* file created by continue in the *apiKey:"  "*.

image:rhads-ai/rhdevspaces/maas-key.png[width=80%]

** Copy the *Endpoint URL* and paste it into the *config.yaml* file created by continue in the *apiBase:* and add at the end of the route */v1*


* Save the file and close the  *Config update* message

image:rhads-ai/rhdevspaces/devspaces-config-updated.png[width=60%]


=== Testing the AI Agent

* Click on the recent added model *llama-3-2-3b*, to include it on the continue chat:  

image:rhads-ai/rhdevspaces/continue-llama-select.png[width=60%]

* Ask the AI assistant any questions, such as:

image:rhads-ai/rhdevspaces/continue-chat1.png[width=60%]

* Learn about the current implementation. Let's start with the *search_agent.py* file

[source,bash,role=execute,subs=attributes+]
----
@search_agent.py  tell me about this file
----

Note: ensure you are adding @search_agent.py at the beginning of the prompt

image:rhads-ai/rhdevspaces/continue-chat2.png[width=100%]


* Learn about the current implementation. Let's start with the *requirements.txt* file

[source,bash,role=execute,subs=attributes+]
----
@requirements.txt  tell me about this file
----

Note: ensure you are adding @requirements.txt *at the beginning of the prompt

image:rhads-ai/rhdevspaces/continue-chat3.png[width=100%]

*Congratulations*, using a self-hosted LLM, you have successfully configured an *AI Assistant in Red Hat OpenShift Dev Spaces* and learn about the AI Agent implementation.


== Configure an *MCP server in the AI Assistant*

The AI assistant will use *Tavily* for the *tools*, such as Web Search. We want to include the *Tavily MCP server* in our Development environment to learn about the AI agent development.

* Click on continue chat and click on the  *tools icon*:

image:rhads-ai/rhdevspaces/mcp-configure-tools.png[width=60%]

* In the tools window, click on the *plus icon*, to configure the *MCP server*.

image:rhads-ai/rhdevspaces/mcp-configure-new.png[width=100%]


* You will see a new file created called: *new-mcp-server.yaml*

* Copy the following content and past it to replace the *whole current content*:

** Copy the content provided, by clicking on the icon:

+
image:rhads-ai/rhads/rhdh-copy-icon.png[width=20%]


[source,bash,role=execute,subs=attributes+]
----
name: Tavily MCP
version: 0.0.1
schema: v1
mcpServers:
  - name: Tavily MCP server
    command: npx
    args:
      - -y
      - mcp-remote 
      - https://mcp.tavily.com/mcp/?tavilyApiKey=
    env: {}
----

* We have already provided you with a configuration to set up *Tavily MCP server*:
** If needed, verify the solution: https://github.com/redhat-ads-tech/rhads-enablement-l3/blob/main/content/modules/ROOT/solutions/rhads-ai/rhdevspaces/tavily-mcp-server.yaml[tavily-mcp-server config file^]

*Note:* Other AI assistants might not use yaml files but JSON files. If you are trying to configure an AI Assistant in the future, the MCP server web page will have an example of how to configure it with other AI assistants. Explore more from https://docs.tavily.com/documentation/mcp[Tavily MCP^]

* Copy the file content and *paste* it into the new-mcp-server.yaml file created by continue.
** You are replacing the *MCP SERVER NAME CONFIG*, *MCP SERVER NAME*, *ARGS*, as it shows on the following picture:

image:rhads-ai/rhdevspaces/mcp-server-config-new.png[width=100%]

=== Access your account on the Tavily website

* Access https://app.tavily.com[https://app.tavily.com^]
* Click on *Login* to create a new account
* Choose your sign up method, as previously selected: *Google*, *GitHub*, *Email address*


image:rhads-ai/rhdevspaces/tavily-web.png[width=100%]

* Click on continue, and once logged in, you will see a screen like this one:

image:rhads-ai/rhdevspaces/tavily-main.png[width=100%]

* Click on add *API KEY* , copy the *API KEY VALUE*:

image:rhads-ai/rhdevspaces/tavily-apikey-view.png[width=100%]

**NOTE: KEEP THE API-KEY SECRET, keep it save, keep it secret. DO NOT PUBLISH IT OR SHARE IT WITH ANYONE, NOT ANY PUBLIC GIT REPOSITORY. THIS IS YOUR PERSONAL API KEY**

* Paste your key to include it after the *?tavilyApiKey=*

image:rhads-ai/rhdevspaces/mcp-server-config-apikey.png[width=100%]

* Save the changes.
* Now, you will see *Tavily MCP server* in the MCP server configuration.

If the server is not yet ready, click on the *refresh* icon:
image:rhads-ai/rhdevspaces/mcp-server-refresh.png[width=70%]


=== Using the *MCP server in the AI Assistant* to learn about Tavily

Tavily provides different tools including Tavily Search and Tavily Extract.

* Click on the *back icon* to go back to the continue chat  

image:rhads-ai/rhdevspaces/mcp-server-back.png[width=60%]

* Ensure the *agent* is selected in the Continue terminal option.

image:rhads-ai/rhdevspaces/mcp-server-select-agent.png[width=60%]

An agent to interact with the *mcp server tools*.

* Ask the AI Agent a questions about *Tavily Search*:

[source,bash,role=execute,subs=attributes+]
----
tell me about tavily search
----
image:rhads-ai/rhdevspaces/mcp-server-chat.png[width=100%]

* Ask the AI Agent a questions about *Tavily Extract*:

[source,bash,role=execute,subs=attributes+]
----
tell me about tavily extract
----
image:rhads-ai/rhdevspaces/mcp-server-chat-extract.png[width=100%]


*Congratulations*, you have successfully configured an *MCP Server in Red Hat OpenShift Dev Spaces* as part of the AI assistant and learn about the AI Agent tools.



